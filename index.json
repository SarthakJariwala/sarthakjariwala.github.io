[{"content":"If you have multiple services listed in your docker-compose file as shown below:\n# example docker-compose.yml version: \u0026#34;3.7\u0026#34; services: test: image: ezconda-test build: context: ./ dockerfile: Dockerfile container_name: ezconda-test dev: image: ezconda-dev build: context: ./ dockerfile: Dockerfile.dev container_name: ezconda-dev To only build and run the dev service listed in the docker-compose.yml file use the following command :\ndocker-compose build dev \u0026amp;\u0026amp; docker-compose run dev bash  Here, bash is the command that we want to run in the container.\n ","permalink":"https://sarthakjariwala.github.io/til/docker/build-run-single-service/","summary":"If you have multiple services listed in your docker-compose file as shown below:\n# example docker-compose.yml version: \u0026#34;3.7\u0026#34; services: test: image: ezconda-test build: context: ./ dockerfile: Dockerfile container_name: ezconda-test dev: image: ezconda-dev build: context: ./ dockerfile: Dockerfile.dev container_name: ezconda-dev To only build and run the dev service listed in the docker-compose.yml file use the following command :\ndocker-compose build dev \u0026amp;\u0026amp; docker-compose run dev bash  Here, bash is the command that we want to run in the container.","title":"Build \u0026 Run a Single Service in docker-compose"},{"content":"I came across codespell via Simon Willison\u0026rsquo;s tweet.\nCodespell is a python library that provides a cli to check and fix common misspellings in text files.\nJust running it against the content of this website, I discovered a few misspellings.\ncodespell content The changes suggested by codespell can be implemented with a -w flag.\n","permalink":"https://sarthakjariwala.github.io/til/python/codespell/","summary":"I came across codespell via Simon Willison\u0026rsquo;s tweet.\nCodespell is a python library that provides a cli to check and fix common misspellings in text files.\nJust running it against the content of this website, I discovered a few misspellings.\ncodespell content The changes suggested by codespell can be implemented with a -w flag.","title":"Codespell - check spelling"},{"content":"Recently, I was using prefect to run a few workflows locally. I wanted to run one of the workflows in parallel using LocalDaskExecutor. That\u0026rsquo;s when I learned that LocalDaskExecutor uses num_workers and DaskExecutor uses n_workers to specify the number of workers.\nWith LocalDaskExecutor from prefect.executors import LocalDaskExecutor if __name__ == \u0026#34;__main__\u0026#34;: flow.run(executor=LocalDaskExecutor(scheduler=\u0026#34;processes\u0026#34;, num_workers=16)) With DaskExecutor from prefect.executors import DaskExecutor if __name__ == \u0026#34;__main__\u0026#34;: flow.run(executor=DaskExecutor(cluster_kwargs={\u0026#34;n_workers\u0026#34;: 16})) ","permalink":"https://sarthakjariwala.github.io/til/prefect/local-dask-executor/","summary":"Recently, I was using prefect to run a few workflows locally. I wanted to run one of the workflows in parallel using LocalDaskExecutor. That\u0026rsquo;s when I learned that LocalDaskExecutor uses num_workers and DaskExecutor uses n_workers to specify the number of workers.\nWith LocalDaskExecutor from prefect.executors import LocalDaskExecutor if __name__ == \u0026#34;__main__\u0026#34;: flow.run(executor=LocalDaskExecutor(scheduler=\u0026#34;processes\u0026#34;, num_workers=16)) With DaskExecutor from prefect.executors import DaskExecutor if __name__ == \u0026#34;__main__\u0026#34;: flow.run(executor=DaskExecutor(cluster_kwargs={\u0026#34;n_workers\u0026#34;: 16})) ","title":"LocalDaskExecutor : num_workers"},{"content":" A comparative look at solar resource availability\n Is solar energy a viable resource for a city like Seattle? My friend recently asked me this question. I believe it\u0026rsquo;s a fair question to ask. Seattle isn\u0026rsquo;t known for its abundance of sunshine. As I write this post, it is a gloomy, cloudy and rainy day in Seattle. For those familiar with Seattle weather, this is not a surprise - fall and winter months are often cloudy and rainy.\nIn this post, we will answer this question by taking a look at the available solar resource for Seattle. We will also compare it against other cities - Boston, Austin and Berlin.\n Note - this post is not a discussion on the differences in the local policies but only the resource availability.\n Where do we get solar resource data? National Renewable Energy Lab (NREL) provides access to the National Solar Radiation Database (NSRDB). NSRDB is collection of hourly and half-hourly values of meteorological data and solar radiation measurements.\nWe will download the data and perform the analysis using Python (You can also use the web interface to download the data, if you prefer).\nTo download the data programmatically, we will need a NREL Developer API key - it is free and the signup only requires your name and email (where you will receive the API key).\nInstallation We will use a Python API that I have created, nrel-dev-api, to programmatically access data and analysis services from NREL. It currently covers all of the solar API endpoints that NREL provides, with future support for other services such as wind, electricity, etc.\npip install --upgrade nrel-dev-api Set your API key from nrel_dev_api import set_nrel_api_key set_nrel_api_key(\u0026#34;YOUR_NREL_API_KEY\u0026#34;) Download NSRDB data To download the data, we need the latitude and longitude for our city of interest as well as the year and time interval of our data. Seattle\u0026rsquo;s latitude and longitude are 47.61 and -122.35, respectively.\nFirst, we will check to see if there are any data available for the specified year, interval and location.\nfrom nrel_dev_api.solar import (download_nsrdb_data, get_nsrdb_download_links) seattle_links = get_nsrdb_download_links(year=2016, interval=60, lat=47.61, lon=-122.35) seattle_links [\u0026#39;https://developer.nrel.gov/api/nsrdb/v2/solar/psm3-download.csv?names=2016\u0026amp;wkt=POINT%28-122.35+47.61%29\u0026amp;interval=60\u0026amp;api_key=yourapikey\u0026amp;email=youremail\u0026#39;] The above get_nsrdb_download_links returns a list of available links for the location and year of interest. Once we have the links, we can pass them to download_nsrdb_data to download the data.\nseattle_hourly_df = download_nsrdb_data(seattle_links[0], email=\u0026#34;YOUR_EMAIL\u0026#34;) The download_nsrdb_data function returns a pandas dataframe containing hourly irradiance, temperature, humidity, pressure, etc. The specific value of interest to us here is the column named \u0026ldquo;GHI\u0026rdquo; which stands for \u0026lsquo;Global Horizontal Irradiance\u0026rsquo;.\n Global horizontal irradiance is the amount of irradiance falling on a surface that is horizontal to the surface of the earth. This value is of particular interest for solar installations.\n We will first resample the \u0026ldquo;GHI\u0026rdquo; data monthly and then sum it up to create monthly global horizontal irrandiance data.\nseattle_ghi_monthly_sum = 1e-3 * seattle_hourly_df[\u0026#34;GHI\u0026#34;].resample(\u0026#34;M\u0026#34;).sum()  Note - the \u0026ldquo;GHI\u0026rdquo; data is in units of Wh/m2 and we are converting it to KWh/m2.\n The resulting dataframe contains the monthly sum of \u0026ldquo;GHI\u0026rdquo; for the year 2016 in Seattle.\n2016-01-31 33.661 2016-02-29 49.141 2016-03-31 90.370 2016-04-30 154.304 2016-05-31 169.188 2016-06-30 198.978 2016-07-31 191.013 2016-08-31 180.028 2016-09-30 126.233 2016-10-31 61.287 2016-11-30 35.059 2016-12-31 45.311 Freq: M, Name: GHI, dtype: float64 We can now repeat the process for Boston and Austin to get the monthly sum of \u0026ldquo;GHI\u0026rdquo; in 2016, boston_ghi_monthly_sum and austin_ghi_monthly_sum (If you want to see the code for Boston and Austin, check out the jupyter notebook).\nLet\u0026rsquo;s put all the monthly GHI data into a single dataframe.\nimport pandas as pd import numpy as np index = [\u0026#34;Jan\u0026#34;, \u0026#34;Feb\u0026#34;, \u0026#34;Mar\u0026#34;, \u0026#34;Apr\u0026#34;, \u0026#34;May\u0026#34;, \u0026#34;Jun\u0026#34;, \u0026#34;Jul\u0026#34;, \u0026#34;Aug\u0026#34;, \u0026#34;Sep\u0026#34;, \u0026#34;Oct\u0026#34;, \u0026#34;Nov\u0026#34;, \u0026#34;Dec\u0026#34;] ghi_df = pd.DataFrame( np.array([ seattle_ghi_monthly_sum.values, boston_ghi_monthly_sum.values, austin_ghi_monthyl_sum.values ]).T, columns=[\u0026#34;Seattle\u0026#34;, \u0026#34;Boston\u0026#34;, \u0026#34;Austin\u0026#34;], index=index ) Finally, let\u0026rsquo;s get GHI data for Berlin. NSRDB doesn\u0026rsquo;t have data for Berlin but we can get it from PVGIS, a tool provided by the European Union science hub. After downloading the data, we can load it as a pandas dataframe.\nberlin_monthly_df = (pd.read_csv(\u0026#34;Monthlydata_52.516_13.377_SA_2016_2016.csv\u0026#34;, sep=\u0026#34;\\t\u0026#34;, skiprows=4, skipfooter=4) .dropna(axis=1) .rename(columns={\u0026#34;H(h)_m\u0026#34; : \u0026#34;monthly_ghi_kWh_m2\u0026#34;}) ) ghi_df[\u0026#34;Berlin\u0026#34;] = berlin_monthly_df[\u0026#34;monthly_ghi_kWh_m2\u0026#34;].values Seattle\tBoston\tAustin\tBerlin Jan\t33.661\t64.608\t111.509\t16.03 Feb\t49.141\t83.216\t135.596\t37.93 Mar\t90.370\t119.618\t153.963\t64.28 Apr\t154.304\t160.144\t162.074\t120.44 May\t169.188\t174.102\t166.599\t182.66 Jun\t198.978\t213.426\t211.531\t173.66 Jul\t191.013\t199.640\t229.077\t151.20 Aug\t180.028\t184.762\t187.451\t142.08 Sep\t126.233\t124.293\t165.196\t117.23 Oct\t61.287\t96.820\t152.188\t40.18 Nov\t35.059\t61.626\t93.345\t25.73 Dec\t45.311\t54.708\t71.597\t15.45 Let\u0026rsquo;s visualize the differences in the irradiance data.\nimport seaborn as sns ax = sns.barplot(data=ghi_df, ci=\u0026#34;sd\u0026#34;) ax.set_ylabel(\u0026#34;KWh/m$^{2}$ per month\u0026#34;) ax.set_title(\u0026#34;Global Horizontal Irradiance\u0026#34;) The barplot shows the average global horizontal irradiance per month (with errorbars representing the standard deviation) for the different cities in our dataframe.\nax = ghi_df.plot( kind=\u0026#34;line\u0026#34;, ylabel=\u0026#34;KWh per m$^{2}$\u0026#34;, title=\u0026#34;Total Monthly Global Horizontal Irradiance\u0026#34;, style=[\u0026#34;o-\u0026#34;, \u0026#34;o-\u0026#34;, \u0026#34;o-\u0026#34;, \u0026#34;o-\u0026#34;] ) The lineplot shows the monthly sum of global horizontal irradiance for the different cities in our dataframe.\n# Annual Global Horizontal Irradiance ghi_df.sum() # KWh/m2 Seattle 1334.573 Boston 1536.963 Austin 1840.126 Berlin 1086.870 dtype: float64 Seattle gets sunlight comparable to Boston but lower than Austin, monthly as well as annually. The fall and winter months receive less sunlight, which is not surprising.\nHowever, when we compare Seattle with Berlin, we see that Seattle gets higher solar irradiance thorughout the year. This is very promising for solar in Seattle (and Washington).\nIn 2020, Germany produced 51 TWh of electricity from solar despite getting less sun than Seattle! That\u0026rsquo;s 10.5% of Germany\u0026rsquo;s electricity generation.\nOf course, there are differences in policies and cost of solar; but looking at the solar resource availability, there is no reason why solar energy can\u0026rsquo;t be more successful in Seattle, Washington and the US in general.\n Thanks for reading! You can find the notebook with the code here.\n","permalink":"https://sarthakjariwala.github.io/posts/sunny-in-seattle/","summary":"A comparative look at solar resource availability\n Is solar energy a viable resource for a city like Seattle? My friend recently asked me this question. I believe it\u0026rsquo;s a fair question to ask. Seattle isn\u0026rsquo;t known for its abundance of sunshine. As I write this post, it is a gloomy, cloudy and rainy day in Seattle. For those familiar with Seattle weather, this is not a surprise - fall and winter months are often cloudy and rainy.","title":"How Sunny is Seattle?"},{"content":" Add scalebars, visualize image distribution, correct outliers, and more.\n This post introduces some of the functionalities in seaborn-image for descriptive, effective and attractive image visualization.\nSeaborn-image is an open source python visualization library for images built on top of matplotlib. It aims to provide a high level API to visualize image data similar to how seaborn provides high level API to visualize tabular data. As the name suggests, seaborn-image is heavily inspired by the seaborn library.\n Installation 2-D Images Add Scalebar Correct Outliers Image Data Distribution   Installation Let\u0026rsquo;s begin by installing seaborn-image\n$ pip install --upgrade seaborn-image and then importing seaborn-image as isns\nimport seaborn_image as isns # set context  isns.set_context(\u0026#34;notebook\u0026#34;) # set global image settings isns.set_image(cmap=\u0026#34;deep\u0026#34;, origin=\u0026#34;lower\u0026#34;) # load sample dataset polymer = isns.load_image(\u0026#34;polymer\u0026#34;) All the functions in seaborn-image are available in a flat namespace.\nisns.set_context() helps us globally change the display contexts (similar to seaborn.set_context()).\nIn addition to the context, we also globally set properties for drawing our image using isns.set_image(). Later, we will also look at globally setting image scalebar properties using isns.set_scalebar().\nUnder the hood, these functions use matplotlib rcParams for customizing displays. You can refer to the docs for more details on settings in seaborn-image.\nLastly, we load a sample polymer dataset from seaborn-image.\n2-D Images Visualizing the image is as simple as calling the imgplot() function with the image data. imgplot() uses matplotlib imshow under the hood but provides easy access to a lot of customizations. We will take a look at a few of the customizations in this blog post.\nBy default, it adds a colorbar and turns off axis ticks. However, that is only beginning to scratch the surface!\nWe can get some basic descriptive statistics about our image data by setting describe=True.\nax = isns.imgplot( polymer, describe=True, # default is False ) No. of Obs. : 65536 Min. Value : -8.2457214 Max. Value : 43.714034999999996 Mean : 7.456410761947062 Variance : 92.02680396572863 Skewness : 0.47745180538933696  You can also use imshow, an alias for imgplot\n Draw a Scalebar Although we know some basic information about our image data, we still do not have any information about the physical size of the features in the image. We can draw a scalebar to rectify it.\nTo add a scalebar to the image we can specify the individul pixel size dx and physical units. Here, the individual pixel is 15 nanometers in physical size. So, we set dx=15 and units=\u0026quot;nm\u0026quot;.\nax = isns.imgplot( polymer, dx=15, # physical size of the pixel units=\u0026#34;nm\u0026#34;, # units  cbar_label=\u0026#34;Height (nm)\u0026#34; # colorbar label to our image )  Note: We only specified the individual pixel size and units, and a scalebar of appropriate size was drawn.\n  Tip: You can change the scalebar properties such as scalebar location, label location, color, etc. globally using isns.set_scalebar()\n Correct for Outliers Real data is never perfect. It is often riddled with outliers and these outliers affect the image display.\n# sample data with outliers pol_outliers = isns.load_image(\u0026#34;polymer outliers\u0026#34;) ax = isns.imgplot(pol_outliers, cbar_label=\u0026#34;Height (nm)\u0026#34;) The above example dataset has a single outlier pixel which is affecting the image display. We can correct for the outliers using the robust parameter in all seaborn-image functions.\nax = isns.imgplot( pol_outliers, robust=True, # set robust plotting perc=(0.5, 99.5), # set the percentile of the data to view cbar_label=\u0026#34;Height (nm)\u0026#34; ) Here, we are setting robust=True and plotting 0.5 to 99.5 percentile of the data (specified using the perc parameter). Doing so appropriately scales the colormap based on the robust percentile specified and also draws colorbar extensions without any additional code.\n Note: You can specify the vmin and vmax parameter to override the robust parameter. See imgplot documentation examples for more details\n Image Data Distribution One of the most important things in image visualization is knowing the distribution of the underlying image data. Here, we are using imghist() to plot a histogram along with the image.\nfig = isns.imghist(polymer, dx=15, units=\u0026#34;nm\u0026#34;, cbar_label=\u0026#34;Height (nm)\u0026#34;)  Note that there are no new parameters required.\n Using histogram along with an appropriate colormap provides additional information about the image data. For instance, from the histogram above, we can see that majority of the data has values less than 30 nm and there are very few values that are close to 40 nm - something that may not be obvious if we look at the image without the histogram.\n Tip: You can change the number of bins using the bins parameter and the orientation of the colorbar and histogram using the orientation parameter. See imghist documentation examples for more details\n Importantly, generating the entire figure - with a histogram matching the colorbar levels, scalebar describing the physical size of the features in the image, colorbar label, hiding axis ticks, etc. - took only one line of code. In essence, this is what seaborn-image aims to provide - a high level API for attractive, descriptive and effective image visualization.\nLastly, this post has only introduced some of the high level API that seaborn-image provides for image visualization. For more details, you can check out the detailed documentation and tutorials and the project on GitHub.\nThanks for reading!\n","permalink":"https://sarthakjariwala.github.io/posts/introducing-seaborn-image/","summary":"Add scalebars, visualize image distribution, correct outliers, and more.\n This post introduces some of the functionalities in seaborn-image for descriptive, effective and attractive image visualization.\nSeaborn-image is an open source python visualization library for images built on top of matplotlib. It aims to provide a high level API to visualize image data similar to how seaborn provides high level API to visualize tabular data. As the name suggests, seaborn-image is heavily inspired by the seaborn library.","title":"Attractive, Effective \u0026 Descriptive Image Visualization in Python"},{"content":"Multi-dimensional image data is, generally speaking, cumbersome to visualize.\nIn scientific imaging (or in most imaging areas), multi-dimensional images are very common. The additional dimension could be anything from the physical 3rd dimension (\u0026ldquo;Z axis\u0026rdquo;), where 2D images are taken at different depths; to the time dimension, where 2D images are taken at different time intervals; to different channels in scientific imaging instruments such as atomic force microscopes or in RGB images.\nWe will use seaborn-image, an open source image visualization library in Python based on matplotlib.\n It is heavily inspired by the popular seaborn library for statistical visualization\n Installation pip install -U seaborn-image  You can find out more about the seaborn-image project on GitHub.\n Load sample 3D data import seaborn_image as isns cells = isns.load_image(\u0026#34;cells\u0026#34;) cells.shape (256, 256, 60) Visualize We will use ImageGrid from seaborn_image to visualize the data. It will plot a series of images on a grid.\nTo begin, we will only plot a few selected slices using the slices keyword argument.\ng = isns.ImageGrid(cells, slices=[10, 20, 30, 40, 50]) By default, the slices are taken along the last axis. However, we can take them along another dimension using the axis keyword argument.\ng = isns.ImageGrid(cells, slices=[10, 20, 30, 40, 50], axis=0) We can also specify different start/stop points as well as step sizes to take using the start, stop and step parameters, respectively.\nIn the code below, we are starting with the 10th slice and going up to the 40th slice with steps of 3.\n The slices and steps are taken over the last axis if not specified.\n g = isns.ImageGrid(cells, start=10, stop=40, step=3) We can also just plot all the images without any indexing or slicing.\ng = isns.ImageGrid(cells, cbar=False, height=1, col_wrap=10)  Note - We altered the height of the individual images and the number of image columns.\n Transformations Finally, we can also apply transformations to the image and visualize it. Here, we will adjust the exposure using the adjust_gamma function from scikit-image.\nWe can achieve this by passing the function object to the map_func parameter. Additional parameters to the function object can be passed as keyword arguments.\nfrom skimage import exposure g = isns.ImageGrid( cells, map_func=exposure.adjust_gamma, # function to map gamma=0.5, # additional keyword for `adjust_gamma` cbar=False, height=1, col_wrap=10) ImageGrid returns a seaborn_image.ImageGrid object and is a figure-level function, i.e. it generates a new matplotlib figure. We can access the figure and all the individual axes using the fig and axes attributes, respectively. This means that for any customizations that are not directly available in seaborn-image (see documentation), we can drop down to matplotlib and use its powerful API.\nOverall, as we have seen throughout this post, seaborn-image allows us to be more productive by providing a high-level API for quick, effective and attractive image data visualization.\nYou can find out more about the seaborn-image project on GitHub.\nThanks for reading!\n ","permalink":"https://sarthakjariwala.github.io/posts/multi-dimension-image-data/","summary":"Multi-dimensional image data is, generally speaking, cumbersome to visualize.\nIn scientific imaging (or in most imaging areas), multi-dimensional images are very common. The additional dimension could be anything from the physical 3rd dimension (\u0026ldquo;Z axis\u0026rdquo;), where 2D images are taken at different depths; to the time dimension, where 2D images are taken at different time intervals; to different channels in scientific imaging instruments such as atomic force microscopes or in RGB images.","title":"Effective Visualization of Multi-Dimension Image Data in Python"},{"content":"Researcher and Data Scientist at Palo Alto Research Center with extensive experience in providing actionable insights from data collected and critically analyzed with scientific rigor.\nInterested in working on solutions combining data science and software expertise with core scientific rigor.\nExpertise in communicating complex data to stakeholders and technical \u0026amp; general audience.\nYou can connect with me on LinkedIn or Twitter.\n","permalink":"https://sarthakjariwala.github.io/about/","summary":"Researcher and Data Scientist at Palo Alto Research Center with extensive experience in providing actionable insights from data collected and critically analyzed with scientific rigor.\nInterested in working on solutions combining data science and software expertise with core scientific rigor.\nExpertise in communicating complex data to stakeholders and technical \u0026amp; general audience.\nYou can connect with me on LinkedIn or Twitter.","title":"About Me"},{"content":"List of some of my projects. You can learn more about the projects below.\n seaborn-image : Open-source image visualization in Python nrel-dev-api : Python API for accessing National Renewable Energy Lab developer resources SQ Web \u0026amp; Desktop App : Calculate solar cell\u0026rsquo;s maximum theoretical efficiency Structure-Property Relationships : Elucidate structure-property relationships in next-generation solar cell technology Solar Techno-Economic Analysis : To design solar powered community library \u0026amp; school in rural Ghana Automated Defect Discovery : Physics-informed deep learning on 2D materials Predictive Peptide Analysis : Machine learning for predictive analysis of self-assembled peptides Data Acquisiton Application : Intuitive platform for acquiring data from scientific hardware/instruments GLabViz : Desktop application for scientific data analysis BZMAN : Business management desktop application BZMAN Website : Flask application facilitating application download and other functionalities Other Open-Source Contributions : matpltolib-scalebar, pyscaffold-interactive, etc.   seaborn-image Creator and maintainer of seaborn-image, an open-source image visualization Python package that provides a high level API for drawing attractive, descriptive and effective images.\nLearn more about the project here.\nGitHub | Docs\nUser Feedback\n nrel-dev-api Creator and maintainer of nrel_dev_api, an open-source Python API for accessing data and services provided by National Renewable Energy Lab (NREL).\nExample Usage:\nfrom nrel_dev_api.solar import SolarResourceData # use address or lat/lon to get information about available solar resource seattle = SolarResourceData(address=\u0026#34;Seattle, WA\u0026#34;) from nrel_dev_api.solar import (download_nsrdb_data, get_nsrdb_download_links) # fetch the available download links seattle_links = get_nsrdb_download_links(year=2016, interval=60, lat=47.61, lon=-122.35) # download using the links to a pandas dataframe seattle_hourly_df = download_nsrdb_data(seattle_links[0], email=\u0026#34;YOUR_EMAIL\u0026#34;) For more information, refer the documentation or article\n Structure-Property Relationships in Novel Solar Energy Materials Used statistical methods in combination with expertise in image \u0026amp; data analysis and domain knowledge to elucidate structure-property relationships in next-generation solar cell technology.\nLearn more about the project in the news coverage and paper.\n SQ Desktop and Web App Developed and deployed a web and desktop application for scientists to calculate the maximum efficiency of a solar cell with various materials under different temperatures.\nWeb application | Desktop application\n Solar Techno-Economic Analysis As a senior technical member of Global Renewable Infrastructure Development (GRID) at the University of Washington :\n  Performed techno-economic analysis using SAM (System Advisor Model) and PySAM for designing a solar-powered school \u0026amp; community library in rural Ghana.\n  Developed remote data tracking system for monitoring the health and performance of installed solar modules and microgrids.\n   Automated Defect Discovery Using Deep Learning Developed, packaged, and published a physics-informed deep learning model to automate the localization, classification, and visualization of defects in 2D materials.\nLearn more about it here.\n Predictive Peptide Analysis Used image processing and machine learning for predictive analysis of self-assembled peptides on various substrates as imaged using Atomic Force Microscope.\nLearn more about the project here\n GLabViz Simplified exploration and analysis of outcoming data from various scientific hardware using an intuitive GUI written in Python and Qt, leading to an improved and efficient experimental and analysis workflow.\nA complete description as well as the application can be found on GitHub.\n Data Acquisition Developed intuitive platforms for data acquisition and control using Qt and Python for various scientific hardware/instruments.\nA complete description of the functionalities of the acquisiton platform can be found on the GitHub repo README.\n BZMAN BZMAN (business manager) - A minimal business management application to simplify daily business activities. It is designed to be minimal and for small business needs.\nIt is open-source and written completely in Python. You find more details on the GitHub repo or website.\nDownload\n BZMAN Website A web application running a Flask backend primarily built to demonstrate the functionalities of BZMAN and facilitate BZMAN application download.\nWebsite\n Other Open-Source Contributions Contributor to several other open-source python packages (selected): matplotlib-scalebar, pooch, PyScaffold-Interactive, Scopefoundry. (GitHub)\n ","permalink":"https://sarthakjariwala.github.io/projects/","summary":"List of some of my projects. You can learn more about the projects below.\n seaborn-image : Open-source image visualization in Python nrel-dev-api : Python API for accessing National Renewable Energy Lab developer resources SQ Web \u0026amp; Desktop App : Calculate solar cell\u0026rsquo;s maximum theoretical efficiency Structure-Property Relationships : Elucidate structure-property relationships in next-generation solar cell technology Solar Techno-Economic Analysis : To design solar powered community library \u0026amp; school in rural Ghana Automated Defect Discovery : Physics-informed deep learning on 2D materials Predictive Peptide Analysis : Machine learning for predictive analysis of self-assembled peptides Data Acquisiton Application : Intuitive platform for acquiring data from scientific hardware/instruments GLabViz : Desktop application for scientific data analysis BZMAN : Business management desktop application BZMAN Website : Flask application facilitating application download and other functionalities Other Open-Source Contributions : matpltolib-scalebar, pyscaffold-interactive, etc.","title":"Projects"}]